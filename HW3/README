Joe Ellis and Jessica Ouyang
HW 3

FILES INCLUDED:
FreeBase/
    Api_Key.txt = The file that holds the api key for FreeBase
    pyFreeBase.py = Python Module for interacting with FreeBase

GoogleWebSearch/
    DownloadProfessorSites.py = Python Script for finding "About" webpages from the professors
    errors_downloading.txt = Any professors that we had errors downloading

RunProblem1.py = The program that runs problem 1 and creates an output file based on FreeBase search

famous_dates_train.txt = File holds the birth dates of famous people, used in training
famous_names_train.txt = File holds the names of famous people, used in training
famous_dates_test.txt = File holds the birth dates of famous people, used in testing
famous_dates_train.txt = File holds the birth names of famous people, used in testing
non_famous_birthdays.txt = File holds the birthdays of non-famous people used in training
non_famous_birthdays_test.txt = File holds the birthdays of non-famous people used in testing
non_famous_people.txt = File holds the names of non-famous people used in training
non_famous_people_test.txt = File holds the names of non-famous people used in testing

requirements.txt = The pip installer file for installing the necessary libraries

unstructured/
	evaluate.py
	get_dates.py
	predictions.txt	

structured/
    evaluate_stuct.py = Evaluate the result of structed data output
    predicted_train.txt = The predicted dates for the train set
    predicted_test.txt = The predicted dates of the test set

--------------------

COMPILATION:

All libraries used come directly with the python distribution except for BeautifulSoup4, which should be installed using
the pip installer.
    - When in the HW3 directory in the terminal type "pip install -r requirements.txt"

--------------------

RUNNING:

Structured Prediction:

- All commands should be run from the HW3 directory

1.) ./RunProblem1.py <name_file> <birth_date_file> <prediction_output_file>
    - Ex: "./RunProblem1.py famous_names_train.txt famous_dates_train.txt structured/predicted_train.txt"
    - This function finds the dates of birth for each of the names, and the outputs the predicted dates to the given
      file

2.) ./structured/evaluate_struct.py <gold_dates_file> <predicted_dates_file>
    - Ex: "./structured/evaluate_struct.py famous_dates_test.txt structured/predicted_test.txt"
    - This function prints out the day-level, month-level, and year-level accuracy of the found results.

Web Scraping:

- All commands should be run from the HW3 directory

1.) ./GoogleWebSearch <person_file>
    - Ex: "./GoogleWebSearch/DownloadProfessorSites.py non_famous_people.txt"

Unstructured prediction:

python unstructured/get_dates.py non_famous_people.txt non_famous_websites/
Optional third argument: year to use as default if unable to predict

python unstructured/evaluate.py non_famous_birthdays.txt unstructured/predictions.txt
This also outputs the average over gold labels.  The average from training can be used as the default year for testing (see above); otherwise the default is the average predicted year.

--------------------

TASK DESCRIPTION:

We used FreeBase for the structured data and professors' webpages for the unstructured data.  For the structured data we used president's and historcal american figures as our training and testing set.  We retrieved the birthdays of these people from wikipedia, and then split the train and test set into 40 people for training and 10 for testing.  For the unstructured data, we got the gold labels from Wikipedia but did not use Wikipedia for predicting the birthdays.  As discussed with Andrew, we only predicted birth years and used 75 professors, with 60 for training and 15 for testing.

--------------------

EXPERIMENTS:

Structured Data:
For the structured data we used FreeBase to find the date of birth of the people that we are training and testing on.  We first search FreeBase for the relevant people, and only accept the most relevant "person" type topic.  We then get the Freebase "id", or path to the proper freebase query, and use the mql query api provided to get the date_of_birth of the desired person.
Our results on the training set of 40 people is 100% accuracy on every possible scale, day, year, and month.  On the testing set we receive 100% accuracy on the year scale, and 90% accuracy on the day and month scale.  This is due to the fact that for "Thomas Paine" there is a disagreement in wikipedia and Freebase as to what his birthday is.

WebScraping:
We attempt to find the approximate birthday of a professor by scraping their webpage automatically using a combination of a spoofed Google Search, and then a homemade bio-page finding algorithm.  We begin by querying google with the professor's name, and then finding the links that are rendered on the first page, (this solution and code was provided on-line at http://stackoverflow.com/questions/1657570/google-search-from-a-python-app).  Using these links we then extract all of the links that have ".edu" within them.  We then download the shortest one, assuming this would be the base homepage of the author.  After this we parse the html page and extract all of the links and hyperlink text available in the homepage.  We check to see if any of the hyperlink text is one of the common link-words for a short biography such as "cv","bio","biosketch", or "about".  If so, we then determine if the url is relative or absolute, and create the url of the biopage that we have found, we then download this webpage and scrape out the text.  If no bio page is found in the links, then we assume that a short bio is provided on the site homepage, and we download the homepage for this person.  One possible problem with this approach, is if the professor is hosting a website outside of his/her school's domain such as "victoria-stodden.com", also some results returned very general pages such as "www.english.columbia.edu", and were not related to any particular professor.  However, for the most part the results were relatively good, and we performed well for an open-ended problem.  We also utilized 75 names from the request of Professor Rosenberg.

Unstructured Data:
We achieve 26.67% accuracy on our test set.  Our main problem is that a lot of professors' websites don't say when they got their degrees on the main page.  Sometimes the information is on an "about me" or "biography" page, but sometimes it isn't on the website at all.  Even with the "about me" pages, there isn't any consistent way of naming those pages.  It would probably work better to download professors' CVs instead of using their websites, but then we would have to scrape the text out of the PDFs.

Right now we use a very simple and naive method for getting graduate dates from the websites: we search for degree words ("B.A.", "Ph.D", etc.) appearing near dates and assume that those dates refer to those degrees.  We find all dates, then check the line in which each date appears, the previous, and finally the following line.  We tried running Stanford CoreNLP on the webpages to clean the html, tokenize, and tag dates, but it took a very long time on some pages.

After extracting the graduation dates, we predict the birth year using very simple rules: we assume that everyone graduates from undergrad at 22 and then immediately starts either a 2-year masters or a 5-year PhD.  With more data, it would make more sense to learn age of graduation from the training set instead of using rules.  
