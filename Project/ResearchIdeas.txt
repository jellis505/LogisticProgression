## Research Ideas.txt ##
Authors: Joe Ellis and Jessica Ouyang
Date: 10/11/2013

Joe's Overall Thoughts:
So overall I am somewhat interested in trying to combine some vision, nlp, and multimodal research into our projects.  This is the direction that I have my thesis, but don't really have a super solid direction yet.  Pretty much most intersted in multimodal information extraction.  Therefore, I think that our interests could have some pretty cool mixtures and headways and these are some of the projects that I have been thinking about.

One resource that we could have to pull from is the News Rover dataset that we have created in the past year.  Pretty much it's 500,000 video news stories from the past year all with transcripts time-aligned and features extracted from the video.

Possible Projects
1.)
On-Line Question Answering with Video
Components:  Visual Analysis, Semantic Analysis, NLP Question Answering, Multi-Modal Entity Clustering
Idea:  As discussed in Watson question answering is cool, but instead of just a machine voicing the answer it would be cool if we could find videos that would answer the questions for the user.  For example, what are Christine Amanpour's thoughts on "blah", and we could find all of the speech we have from this woman and see if we can find answer and show it.

2.) Multimodal Sentiment Analysis
Components: NLP Sentiment Analysis, Visual Sentiment Analysis (our lab is pioneering this work), Audio Sentiment Analysis, Multi-Modal feature integration, NLP information extraction
Idea:  Given a query or a subject we want to be able to judge online what the sentiment of a topic, question, sentence, meme etc is.  This could be done with a variety of the web crawlers like twitter, Youtube, Facebook, Google News,  plus the TV content that we have.  Ability to implement and demonstrate the differences in sentiment across differning mediums (twitter, youtube, TV), and modalities (text, audio, visual, fusion).
Plus: No one has done this yet, and I think alot of people would be interested.

3.) 
Open Information Extraction with Video Content
Components: Entity-Linking across Webscale, On-line Database (Freebase), QUestion Answering, Multi-modal processing of video
Idea: Open Information extraction involves taking text information from a variety of different sources, and then pooling the information together to create a knowledge base. Oren Etzioni at UW is pioneering this work on web-scale, and has a couple of good papers on it.  Much of it is done through extracting elements and actions, "Yankees acuire Pineda", "Obama marries Michelle", etc.  I think it would be cool to create a database of available videos and pictures that expand upon these extracted relationships.  Videos of "Pineda pitching for the yankees", we could also take content from our news dataset to answer some of these questions.

Note:  We can probably scale all of these back somehow to make them doable in three months of whatever, but these are some things i would be especially interested in working on.
